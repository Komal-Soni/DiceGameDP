{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Problem Statement\n",
        "\n",
        "Develop a reinforcement learning agent using dynamic programming methods to solve the Dice game optimally. The agent will learn the optimal policy by iteratively evaluating and improving its strategy based on the state-value function and the Bellman equations."
      ],
      "metadata": {
        "id": "7Uh03PdwA9-T"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "utCCy98e7aft"
      },
      "source": [
        "# Scenario:\n",
        "A player rolls a 6-sided die with the objective of reaching a score of **exactly** 100. On each turn, the player can choose to stop and keep their current score or continue rolling the die. If the player rolls a 1, they lose all points accumulated in that turn and the turn ends. If the player rolls any other number (2-6), that number is added to their score for that turn. The game ends when the player decides to stop and keep their score OR when the player's score reaches 100. The player wins if they reach a score of exactly 100, and loses if they roll a 1 when their score is below 100.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Environment Details\n",
        "\n",
        "* The environment consists of a player who can choose to either roll a 6-sided die or stop at any point.\n",
        "* The player starts with an initial score (e.g., 0) and aims to reach a score of exactly 100.\n",
        "* If the player rolls a 1, they lose all points accumulated in that turn and the turn ends. If they roll any other number (2-6), that number is added to their score for that turn.\n",
        "* The goal is to accumulate a total of exactly 100 points to win, or to stop the game before reaching 100 points.\n",
        "\n",
        "#### States\n",
        "* State s: Represents the current score of the player, ranging from 0 to 100.\n",
        "* Terminal States:\n",
        "    * State s = 100: Represents the player winning the game by reaching the goal of 100 points.\n",
        "    * State s = 0: Represents the player losing all points accumulated in the turn due to rolling a 1.\n",
        "\n",
        "\n",
        "#### Actions\n",
        "* Action a: Represents the decision to either \"roll\" the die or \"stop\" the game at the current score.\n",
        "* The possible actions in any state s are either \"roll\" or \"stop\".\n"
      ],
      "metadata": {
        "id": "9tvyyPpNB8M5"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "851webtI7aft"
      },
      "source": [
        "# Expected Outcomes:\n",
        "1.\tUse dynamic programming methods value iteration, policy improvement and policy evaluation to find the optimal policy for the Dice Game.\n",
        "2.\tImplement an epsilon-greedy policy for action selection during training to balance exploration and exploitation.\n",
        "3.\tEvaluate the agent's performance in terms of the probability of reaching exactly 100 points after learning the optimal policy.\n",
        "4.\tUse the agent's policy as the best strategy for different betting scenarios within the problem.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Code Execution"
      ],
      "metadata": {
        "id": "8GRKO9n6EeWe"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fxpP5E667afu"
      },
      "source": [
        "### Initialize constants"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "ewHMwHxk7afu"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "# Constants\n",
        "goal = 100\n",
        "gamma = 1.0\n",
        "prob_roll = 0.6\n",
        "\n",
        "# Initialize value function and policy\n",
        "V = np.zeros(goal + 1)\n",
        "policy = np.zeros(goal + 1, dtype=int)  # 0 for \"stop\", 1 for \"roll\""
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Design a DiceGame Environment"
      ],
      "metadata": {
        "id": "HroEzPwhQkwq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Code for Dataset loading and print dataset statistics along with reward function\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class DiceGameEnvironment:\n",
        "    def __init__(self, goal=100, prob_roll=0.6):\n",
        "        self.goal = goal  # Goal score to reach\n",
        "        self.prob_roll = prob_roll  # Probability of rolling the die successfully\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"Reset the environment to starting state.\"\"\"\n",
        "        self.current_score = 0\n",
        "        self.done = False\n",
        "\n",
        "    def step(self, action):\n",
        "        \"\"\"Take a step in the environment based on the action.\"\"\"\n",
        "        if self.done:\n",
        "            raise ValueError(\"Cannot step in a completed episode. Call reset() to start a new episode.\")\n",
        "\n",
        "        reward = 0\n",
        "        if action == 'roll':\n",
        "            roll_result = np.random.randint(1, 7)  # Roll a 6-sided die\n",
        "            if roll_result == 1:\n",
        "                reward = -self.current_score  # Lose all points accumulated in this turn\n",
        "                self.current_score = 0\n",
        "                self.done = True\n",
        "            else:\n",
        "                self.current_score += roll_result\n",
        "                if self.current_score >= self.goal:\n",
        "                    reward = self.current_score  # Win by reaching the goal\n",
        "                    self.done = True\n",
        "        elif action == 'stop':\n",
        "            reward = self.current_score  # Keep current score and end the episode\n",
        "            self.done = True\n",
        "        else:\n",
        "            raise ValueError(\"Invalid action. Choose either 'roll' or 'stop'.\")\n",
        "\n",
        "        return self.current_score, reward, self.done\n",
        "\n",
        "    def get_state(self):\n",
        "        \"\"\"Return the current score/state of the environment.\"\"\"\n",
        "        return self.current_score\n",
        "\n",
        "    def is_done(self):\n",
        "        \"\"\"Check if the game is finished.\"\"\"\n",
        "        return self.done\n"
      ],
      "metadata": {
        "id": "Uc7EP7ZXQsn5"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KxOlvFZ7afv"
      },
      "source": [
        "# Policy Iteration Function Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "us3gUgp57afv"
      },
      "outputs": [],
      "source": [
        "def policy_iteration(env, gamma=1.0):\n",
        "    num_states = env.goal + 1  # States range from 0 to goal\n",
        "    num_actions = 2  # 'roll' or 'stop'\n",
        "\n",
        "    # Initialize random policy and value function\n",
        "    policy = np.random.randint(0, num_actions, size=num_states)  # Random policy\n",
        "    V = np.zeros(num_states)  # Value function\n",
        "\n",
        "    def calculate_state_value(state, action):\n",
        "        \"\"\"Calculate the value of a state-action pair.\"\"\"\n",
        "        if action == 1:  # Action 1 corresponds to 'stop'\n",
        "            return state  # Value of 'stop' action is the current score\n",
        "        elif action == 0:  # Action 0 corresponds to 'roll'\n",
        "            if state >= env.goal:\n",
        "                return state  # If already at or above goal, returning the current state\n",
        "            else:\n",
        "                roll_values = [state + roll for roll in range(2, 7)]  # Possible roll outcomes\n",
        "                roll_values = [min(value, env.goal) for value in roll_values]  # Cap at goal\n",
        "                roll_values.append(0)  # Value of rolling a 1 is losing all points\n",
        "                return np.sum([(1/6) * (env.prob_roll * value) for value in roll_values])\n",
        "        else:\n",
        "            raise ValueError(\"Invalid action. Choose either 0 for 'roll' or 1 for 'stop'.\")\n",
        "\n",
        "    def policy_evaluation(policy):\n",
        "        \"\"\"Evaluate the value function for the given policy.\"\"\"\n",
        "        while True:\n",
        "            delta = 0\n",
        "            for s in range(1, env.goal):  # Evaluate all states from 1 to goal-1\n",
        "                old_v = V[s]\n",
        "                V[s] = calculate_state_value(s, policy[s])\n",
        "                delta = max(delta, abs(old_v - V[s]))\n",
        "            if delta < 1e-6:\n",
        "                break\n",
        "\n",
        "    def policy_improvement():\n",
        "        \"\"\"Improve the policy based on the current value function.\"\"\"\n",
        "        policy_stable = True\n",
        "        for s in range(1, env.goal):  # Improve policy for all states from 1 to goal-1\n",
        "            old_action = policy[s]\n",
        "            action_values = [calculate_state_value(s, 0), calculate_state_value(s, 1)]\n",
        "            policy[s] = np.argmax(action_values)\n",
        "            if old_action != policy[s]:\n",
        "                policy_stable = False\n",
        "        return policy_stable\n",
        "\n",
        "    while True:\n",
        "        policy_evaluation(policy)\n",
        "        if policy_improvement():\n",
        "            break\n",
        "\n",
        "    return policy, V\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sif2YH-B7afv"
      },
      "source": [
        "# Value Iteration Function Definition"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "UOlu2Jzl7afv"
      },
      "outputs": [],
      "source": [
        "def value_iteration(env, gamma=1.0, theta=1e-6):\n",
        "    num_states = env.goal + 1  # States range from 0 to goal\n",
        "    num_actions = 2  # 'roll' or 'stop'\n",
        "\n",
        "    # Initialize value function\n",
        "    V = np.zeros(num_states)  # Value function\n",
        "\n",
        "    def calculate_state_value(state, action):\n",
        "        \"\"\"Calculate the value of a state-action pair.\"\"\"\n",
        "        if action == 1:  # Action 1 corresponds to 'stop'\n",
        "            return state  # Value of 'stop' action is the current score\n",
        "        elif action == 0:  # Action 0 corresponds to 'roll'\n",
        "            if state >= env.goal:\n",
        "                return state  # If already at or above goal, returning the current state\n",
        "            else:\n",
        "                roll_values = [state + roll for roll in range(2, 7)]  # Possible roll outcomes\n",
        "                roll_values = [min(value, env.goal) for value in roll_values]  # Cap at goal\n",
        "                roll_values.append(0)  # Value of rolling a 1 is losing all points\n",
        "                return np.sum([(1/6) * (env.prob_roll * value) for value in roll_values])\n",
        "        else:\n",
        "            raise ValueError(\"Invalid action. Choose either 0 for 'roll' or 1 for 'stop'.\")\n",
        "\n",
        "    while True:\n",
        "        delta = 0\n",
        "        for s in range(1, env.goal):  # Evaluate all states from 1 to goal-1\n",
        "            v = V[s]\n",
        "            action_values = [calculate_state_value(s, 0), calculate_state_value(s, 1)]\n",
        "            V[s] = max(action_values)\n",
        "            delta = max(delta, abs(v - V[s]))\n",
        "        if delta < theta:\n",
        "            break\n",
        "\n",
        "    # Extract policy from value function\n",
        "    policy = np.zeros(num_states, dtype=int)\n",
        "    for s in range(1, env.goal):\n",
        "        action_values = [calculate_state_value(s, 0), calculate_state_value(s, 1)]\n",
        "        policy[s] = np.argmax(action_values)\n",
        "\n",
        "    return policy, V"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r5flz-WD7afw"
      },
      "source": [
        "# Executing Policy Iteration and Value Iteration Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "GMgBw1G77afw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21fbb70d-62a3-4fa0-9f00-c46e06c96585"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Policy Iteration:\n",
            "Optimal Policy (Policy Iteration):\n",
            "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "Optimal Value Function (Policy Iteration):\n",
            "[ 0.   2.5  3.   3.5  4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n",
            " 14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.\n",
            " 28.  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.\n",
            " 42.  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.\n",
            " 56.  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.\n",
            " 70.  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.\n",
            " 84.  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.\n",
            " 98.  99.   0. ]\n",
            "\n",
            "Value Iteration:\n",
            "Optimal Policy (Value Iteration):\n",
            "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0]\n",
            "Optimal Value Function (Value Iteration):\n",
            "[ 0.   2.5  3.   3.5  4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n",
            " 14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.\n",
            " 28.  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.\n",
            " 42.  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.\n",
            " 56.  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.\n",
            " 70.  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.\n",
            " 84.  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.\n",
            " 98.  99.   0. ]\n"
          ]
        }
      ],
      "source": [
        "#Simulate the game for 100 states. Use the learned policy to get the actions.\n",
        "#when its roll, randomly generate a number to find the reward.\n",
        "#when its stop, get the respective reward\n",
        "#determine the total cumulative reward\n",
        "\n",
        "# Create the environment\n",
        "env = DiceGameEnvironment(goal=100, prob_roll=0.6)\n",
        "\n",
        "# Perform Policy Iteration\n",
        "print(\"Policy Iteration:\")\n",
        "policy_pi, V_pi = policy_iteration(env)\n",
        "print(\"Optimal Policy (Policy Iteration):\")\n",
        "print(policy_pi)\n",
        "print(\"Optimal Value Function (Policy Iteration):\")\n",
        "print(V_pi)\n",
        "\n",
        "# Perform Value Iteration\n",
        "print(\"\\nValue Iteration:\")\n",
        "policy_vi, V_vi = value_iteration(env)\n",
        "print(\"Optimal Policy (Value Iteration):\")\n",
        "print(policy_vi)\n",
        "print(\"Optimal Value Function (Value Iteration):\")\n",
        "print(V_vi)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaaPfNNa7afw"
      },
      "source": [
        "### Print the Learned Optimal Policy, Optimal Value Function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "BisSE1d27afw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7c850b84-f12f-4acd-9314-7b281200d1a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Learned Optimal Policy and Optimal Value Function:\n",
            "Policy Iteration - Optimal Policy:\n",
            "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]\n",
            "Policy Iteration - Optimal Value Function:\n",
            "[ 0.   2.5  3.   3.5  4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n",
            " 14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.\n",
            " 28.  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.\n",
            " 42.  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.\n",
            " 56.  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.\n",
            " 70.  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.\n",
            " 84.  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.\n",
            " 98.  99.   0. ]\n",
            "\n",
            "Value Iteration - Optimal Policy:\n",
            "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0]\n",
            "Value Iteration - Optimal Value Function:\n",
            "[ 0.   2.5  3.   3.5  4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n",
            " 14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.\n",
            " 28.  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.\n",
            " 42.  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.\n",
            " 56.  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.\n",
            " 70.  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.\n",
            " 84.  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.\n",
            " 98.  99.   0. ]\n"
          ]
        }
      ],
      "source": [
        "# Printing the learned optimal policy and optimal value function\n",
        "print(\"\\nLearned Optimal Policy and Optimal Value Function:\")\n",
        "print(\"Policy Iteration - Optimal Policy:\")\n",
        "print(policy_pi)\n",
        "print(\"Policy Iteration - Optimal Value Function:\")\n",
        "print(V_pi)\n",
        "print(\"\\nValue Iteration - Optimal Policy:\")\n",
        "print(policy_vi)\n",
        "print(\"Value Iteration - Optimal Value Function:\")\n",
        "print(V_vi)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aGUqPYqF7afw"
      },
      "source": [
        "# Change in environment details\n",
        "\n",
        "Consider the following scenario:\n",
        "1. What happens if we change the goal score to 50 instead of 100? How does it affect the optimal policy and value function?\n",
        "2. How would the optimal policy and value function change if the die had 8 sides instead of 6? Assume the outcomes range from 0 to 7, with each outcome having a probability of 1/8.\n",
        "3. Experiment with different discount factors (e.g., 0.9, 0.95). How does discounting future rewards impact the optimal policy and value function?\n",
        "4. Create a heatmap or line plot to visualize the value function over different states. How does the value function change as the state approaches the goal?\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "3bf81q887afx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac9cfce3-7b1d-4a16-ecbb-232ad0683151"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Optimal Policy (Value Iteration, Goal 50):\n",
            "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 0]\n",
            "Optimal Value Function (Value Iteration, Goal 50):\n",
            "[ 0.   2.5  3.   3.5  4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n",
            " 14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.\n",
            " 28.  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.\n",
            " 42.  43.  44.  45.  46.  47.  48.  49.   0. ]\n"
          ]
        }
      ],
      "source": [
        "# Create the environment with goal 50\n",
        "env_goal_50 = DiceGameEnvironment(goal=50, prob_roll=0.6)\n",
        "\n",
        "# Perform Value Iteration\n",
        "policy_vi_goal_50, V_vi_goal_50 = value_iteration(env_goal_50)\n",
        "\n",
        "# Printing the learned optimal policy and optimal value function for goal 50\n",
        "print(\"\\nOptimal Policy (Value Iteration, Goal 50):\")\n",
        "print(policy_vi_goal_50)\n",
        "print(\"Optimal Value Function (Value Iteration, Goal 50):\")\n",
        "print(V_vi_goal_50)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def calculate_state_value(state, action, env):\n",
        "    \"\"\"Calculate the value of a state-action pair for an 8-sided die.\"\"\"\n",
        "    if action == 1:  # Action 1 corresponds to 'stop'\n",
        "        return state  # Value of 'stop' action is the current score\n",
        "    elif action == 0:  # Action 0 corresponds to 'roll'\n",
        "        if state >= env.goal:\n",
        "            return state  # If already at or above goal, returning the current state\n",
        "        else:\n",
        "            roll_values = [state + roll for roll in range(1, 8)]  # Possible roll outcomes 1 to 7\n",
        "            roll_values = [min(value, env.goal) for value in roll_values]  # Cap at goal\n",
        "            roll_values.append(0)  # Value of rolling a 0 is losing all points\n",
        "            return np.sum([(1/8) * (env.prob_roll * value) for value in roll_values])\n",
        "    else:\n",
        "        raise ValueError(\"Invalid action. Choose either 0 for 'roll' or 1 for 'stop'.\")\n",
        "# Create the environment with an 8-sided die\n",
        "env_8_sided_die = DiceGameEnvironment(goal=100, prob_roll=0.8)\n",
        "\n",
        "# Perform Value Iteration\n",
        "policy_vi_8_sided, V_vi_8_sided = value_iteration(env_8_sided_die)\n",
        "\n",
        "# Printing the learned optimal policy and optimal value function for an 8-sided die\n",
        "print(\"\\nOptimal Policy (Value Iteration, 8-Sided Die):\")\n",
        "print(policy_vi_8_sided)\n",
        "print(\"Optimal Value Function (Value Iteration, 8-Sided Die):\")\n",
        "print(V_vi_8_sided)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3QTCHn_rWkt",
        "outputId": "cb557d99-1695-44ae-e973-ed09722cddc9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Optimal Policy (Value Iteration, 8-Sided Die):\n",
            "[0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0]\n",
            "Optimal Value Function (Value Iteration, 8-Sided Die):\n",
            "[ 0.          3.33333333  4.          4.66666667  5.33333333  6.\n",
            "  6.66666667  7.33333333  8.          9.         10.         11.\n",
            " 12.         13.         14.         15.         16.         17.\n",
            " 18.         19.         20.         21.         22.         23.\n",
            " 24.         25.         26.         27.         28.         29.\n",
            " 30.         31.         32.         33.         34.         35.\n",
            " 36.         37.         38.         39.         40.         41.\n",
            " 42.         43.         44.         45.         46.         47.\n",
            " 48.         49.         50.         51.         52.         53.\n",
            " 54.         55.         56.         57.         58.         59.\n",
            " 60.         61.         62.         63.         64.         65.\n",
            " 66.         67.         68.         69.         70.         71.\n",
            " 72.         73.         74.         75.         76.         77.\n",
            " 78.         79.         80.         81.         82.         83.\n",
            " 84.         85.         86.         87.         88.         89.\n",
            " 90.         91.         92.         93.         94.         95.\n",
            " 96.         97.         98.         99.          0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the environment with default parameters\n",
        "env_discount_09 = DiceGameEnvironment(goal=100, prob_roll=0.6)\n",
        "env_discount_095 = DiceGameEnvironment(goal=100, prob_roll=0.6)\n",
        "\n",
        "# Perform Value Iteration with discount factor 0.9\n",
        "policy_vi_discount_09, V_vi_discount_09 = value_iteration(env_discount_09, gamma=0.9)\n",
        "\n",
        "# Perform Value Iteration with discount factor 0.95\n",
        "policy_vi_discount_095, V_vi_discount_095 = value_iteration(env_discount_095, gamma=0.95)\n",
        "\n",
        "# Printing the learned optimal policies and optimal value functions for different discount factors\n",
        "print(\"\\nOptimal Policy (Value Iteration, Discount Factor 0.9):\")\n",
        "print(policy_vi_discount_09)\n",
        "print(\"Optimal Value Function (Value Iteration, Discount Factor 0.9):\")\n",
        "print(V_vi_discount_09)\n",
        "\n",
        "print(\"\\nOptimal Policy (Value Iteration, Discount Factor 0.95):\")\n",
        "print(policy_vi_discount_095)\n",
        "print(\"Optimal Value Function (Value Iteration, Discount Factor 0.95):\")\n",
        "print(V_vi_discount_095)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcmI2KLmrZw0",
        "outputId": "d18edf98-0d2b-4d27-ec91-e0741e95a747"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Optimal Policy (Value Iteration, Discount Factor 0.9):\n",
            "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0]\n",
            "Optimal Value Function (Value Iteration, Discount Factor 0.9):\n",
            "[ 0.   2.5  3.   3.5  4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n",
            " 14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.\n",
            " 28.  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.\n",
            " 42.  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.\n",
            " 56.  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.\n",
            " 70.  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.\n",
            " 84.  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.\n",
            " 98.  99.   0. ]\n",
            "\n",
            "Optimal Policy (Value Iteration, Discount Factor 0.95):\n",
            "[0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
            " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0]\n",
            "Optimal Value Function (Value Iteration, Discount Factor 0.95):\n",
            "[ 0.   2.5  3.   3.5  4.   5.   6.   7.   8.   9.  10.  11.  12.  13.\n",
            " 14.  15.  16.  17.  18.  19.  20.  21.  22.  23.  24.  25.  26.  27.\n",
            " 28.  29.  30.  31.  32.  33.  34.  35.  36.  37.  38.  39.  40.  41.\n",
            " 42.  43.  44.  45.  46.  47.  48.  49.  50.  51.  52.  53.  54.  55.\n",
            " 56.  57.  58.  59.  60.  61.  62.  63.  64.  65.  66.  67.  68.  69.\n",
            " 70.  71.  72.  73.  74.  75.  76.  77.  78.  79.  80.  81.  82.  83.\n",
            " 84.  85.  86.  87.  88.  89.  90.  91.  92.  93.  94.  95.  96.  97.\n",
            " 98.  99.   0. ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Generate states and corresponding values\n",
        "states = np.arange(0, 101)\n",
        "values = V_vi  # Replace with the optimal value function of your choice\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(states, values, marker='o', linestyle='-', color='b')\n",
        "plt.title('Optimal Value Function')\n",
        "plt.xlabel('State (Score)')\n",
        "plt.ylabel('Value')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 564
        },
        "id": "VFZxm1JbrbxL",
        "outputId": "aec3a43e-ea38-4873-a91e-feacd91c8145"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Conclusion\n",
        "\n",
        "In tackling the Dice game problem, the approach hinges on utilizing dynamic programming techniques, specifically value iteration and policy iteration, to derive an optimal strategy for the player. Initially, with a goal score of 100 and a 6-sided die where rolling a 1 resets the score to zero, the optimal policy focuses on balancing risk and reward to reach exactly 100 points. Value iteration computes the optimal state values iteratively, considering both rolling the die and stopping at each score level. This approach ensures that the agent learns to maximize the probability of achieving the exact goal without risking losing accumulated points.\n",
        "\n",
        "After altering parameters such as reducing the goal score to 50 or increasing the die sides to 8 with outcomes from 0 to 7, the optimal strategy adapts accordingly. A lower goal score increases the likelihood of achieving it, leading to more conservative play. Conversely, an 8-sided die broadens the range of potential outcomes, necessitating adjustments in the state-value calculations to account for different roll probabilities and their associated rewards or penalties.\n",
        "\n",
        "Experimenting with discount factors like 0.9 or 0.95 further explores the impact of future reward discounting on the optimal policy. A higher discount factor favors immediate rewards, potentially encouraging more aggressive play to reach the goal sooner, while a lower factor promotes more cautious strategies to ensure the preservation of accumulated points over longer horizons.\n",
        "\n",
        "Visualizing the value function through plots or heatmaps provides insights into how the value of each state evolves as the player approaches the goal. This visualization aids in understanding the critical states where decisions between rolling and stopping are most pivotal.\n",
        "\n",
        "In conclusion, the best approach for solving the Dice game problem involves dynamic programming methods tailored to the specific game parameters. By iteratively refining the state-value function and policy, adjusting strategies to accommodate parameter changes ensures the agent's adaptability and effectiveness in achieving the optimal outcome under varying conditions."
      ],
      "metadata": {
        "id": "xOQk0LcUImEW"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}